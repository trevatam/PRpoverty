---
title: "Puerto Rico Poverty Project"
author: "Treva Tam, Sneha Mani, and Nana Adjeiwaa-Manu"
date: "5 May 2020"
header-includes:
 - \usepackage{dcolumn}
 - \usepackage{booktabs}
 - \usepackage{longtable}
 - \usepackage{array}
 - \usepackage{multirow}
 - \usepackage{wrapfig}
 - \usepackage{float}
 - \usepackage{colortbl}
 - \usepackage{pdflscape}
 - \usepackage{tabu}
 - \usepackage{threeparttable}
 - \usepackage{threeparttablex}
 - \usepackage[normalem]{ulem}
 - \usepackage{makecell}
 - \usepackage{xcolor}
output: rmarkdown::github_document
always_allow_html: true
---
  
This project aims to predict poverty for Costa Rican households using household level data. Using information about a family's observable household attributes like the material of their walls and floor, or if the household has access to drinking water, we attempt to classify their poverty level and predict their level of need. In countries where a large proportion of the population are not part of the formal labor market and are dependent on multiple sources for their income and sustenance, targeting of social programs becomes challenging. In Latin America, the Proxy Means Test is a method used to verify income qualification to target social sector schemes. We use an alternate machine learning approach to verify the income qualification and improve the quality of targeting based on household level factors.   

The project is divided into three parts:

1.  Data cleaning  
2.  Analyzing the data using a standard regression framework  
3.  Analyzing the data using a machine learning approach    

# Data Cleaning

The Inter-American Development Bank provides data at a household level. Using this data, the goal is to classify household into four poverty categories in the variable `Target` (1 = extreme poverty, 2 = moderate poverty, 3 = vulnerable households and 4 = non vulnerable households). Since our research question is at the household level, we collapse the original 9,557 observation dataset, which included several members in one household, to only the head of household. This leaves us with a final dataset of 2,970 unique households.

The variables we choose to focus on are household level attributes like asset ownership, materials used in the walls/floors/ceilings, sanitary and drinking water facilities, and education levels. We combine the variables for years of education for the female head of house and male head of house variables into one variable - years of education for the head of house. The origial dataset has very little missing data except for the variable, number of tablets the household owns. This variable is excluded. After eliminating duplicate variables and focusing on the household level variables, the dataset consists of 60 variables describing the characteristics of the household and an outcome variable classifying the level of poverty.

```{r settings, echo=F, include=F, results='hide'}  
rm(list = ls())
set.seed(2345)
library(arm)
library(dplyr)
library(tidyverse)
library(foreign)
library(knitr)
library(glmnet)
library(leaps)
library(MASS)
library(stargazer)
library(kableExtra)
library(Matrix)
library(caret)
library(nnet)
library(randomForest)
library(ggplot2)     # to plot
library(gridExtra)   # to put more
library(grid)        # plot together
library(ggRandomForests)
```

```{r c2, echo=F, include=F, results='hide'}
base <- read.csv("train.csv", header=TRUE)
head(base)
```

```{r c3, include=F}
#selecting variables from main dataset
data<-base %>% dplyr::select (Id,
                       idhogar,
                       hogar_total,
                       Target,
                       rooms,  
v14a,  
refrig,  
v18q1,  
r4t3,  
paredblolad,  
paredzocalo,  
paredpreb,  
pareddes,  
paredmad,  
paredzinc,  
paredfibras,  
paredother,  
pisomoscer,  
pisocemento,  
pisoother,  
pisonatur,  
pisonotiene,  
pisomadera,  
techozinc,  
techoentrepiso,  
techocane,  
techootro,  
cielorazo,  
abastaguadentro,  
abastaguafuera,  
abastaguano,  
public,  
planpri,  
noelec,  
coopele,  
sanitario1,  
sanitario2,  
sanitario3,  
sanitario5,  
sanitario6,  
energcocinar1,  
energcocinar2,  
energcocinar3,  
energcocinar4,  
hogar_nin,  
hogar_adul,  
hogar_mayor,  
edjefe,  
edjefa,  
meaneduc,  
bedrooms,  
tipovivi1,  
tipovivi2,  
tipovivi3,  
computer,  
television,  
mobilephone,  
qmobilephone,  
lugar1,  
lugar2,  
lugar3,  
lugar4,  
lugar5,  
lugar6,  
age,
parentesco1
)

n_distinct(base$idhogar)

data<-data%>% filter(parentesco1==1) ##only keeping one observation per household and keeping data based on the household head. 
```

```{r c4, echo=F, include=F, results='hide'}
#Rename variables for ease of use
master<-data%>% rename(rooms=rooms , 
toilet=v14a , 
fridge=refrig , 
tablet=v18q1 , 
totperson=r4t3 , 
wallbrick=paredblolad , 
wallsocket=paredzocalo , 
wallcement=paredpreb , 
wallwaster=pareddes , 
wallwood=paredmad , 
wallzinc=paredzinc , 
wallnatural=paredfibras , 
wallother=paredother , 
floormosaic=pisomoscer , 
floorcement=pisocemento , 
floorother=pisoother , 
floornatural=pisonatur , 
floorno=pisonotiene , 
floorwood=pisomadera , 
roofmetal=techozinc , 
roofcement=techoentrepiso , 
roofnatural=techocane , 
roofother=techootro , 
ceiling=cielorazo , 
waterindoor=abastaguadentro , 
wateroutdoor=abastaguafuera , 
waterno=abastaguano , 
elecpub=public , 
elecpvt=planpri , 
elecno=noelec , 
eleccoop=coopele , 
toiletno=sanitario1 , 
toiletsewer=sanitario2 , 
toiletseptic=sanitario3 , 
toilethole=sanitario5 , 
toiletother=sanitario6 , 
energyno=energcocinar1 , 
energyelec=energcocinar2 , 
energygas=energcocinar3 , 
energywood=energcocinar4 , 
children=hogar_nin , 
adults=hogar_adul , 
totalmem=hogar_total,
morethan65=hogar_mayor , 
edumalehoh=edjefe , 
edufemhoh=edjefa , 
avgeduadults=meaneduc , 
bedrooms=bedrooms , 
houseown=tipovivi1 , 
houseloan=tipovivi2 , 
houserented=tipovivi3 , 
computer=computer , 
tv=television , 
mobile=mobilephone , 
rcentral=lugar1 , 
rchorotega=lugar2 , 
rpaca=lugar3 , 
rbrunca=lugar4 , 
rhetarat=lugar5 , 
rhuetarnorte=lugar6 , 
age=age
)
```

```{r c5, echo=F,include=F, results='hide'}
#Dropping Variables and Observations based on Missing
##count the number of missing observations
na_count<-sapply(master, function(y) sum(is.na(y)))
na_count<-data.frame(na_count)
na_count<-na_count%>%add_rownames()
na_count<-rename(na_count,"Missing observations"=na_count)

##tablet has 2318 missing observations so we drop it
master<-master%>% dplyr::select(-tablet)
master<-drop_na(master) #dropped 3 observations for a total of 2,970
```

```{r c6, include=F}
#recode edufemhoh and edumalehoh, "no" as 0 years and "yes" as 1 year
master1 <- master %>%
  mutate(edufemhoh = recode(edufemhoh,
                            'no' = '0',
                            'yes' = '1'),
         edumalehoh = recode(edumalehoh,
                             'no' = '0',
                             'yes' = '1'))
#levels do not align with correct values, make into character before numeric
master1$edufemhoh <- as.numeric(as.character(master1$edufemhoh))
master1$edumalehoh <- as.numeric(as.character(master1$edumalehoh))
#combine edufem and edumale into one variable for head of household education
master1$eduhoh <- master1$edufemhoh + master1$edumalehoh
master1$Target <- as.factor(master1$Target)

master2<-master1%>% dplyr::select(-Id,-idhogar,-edumalehoh,-edufemhoh,-parentesco1) ##Use for regression purposes, get rid of variables no longer needed to use and since we used "parentesco1" as an indicator to choose our observations

```

## Preparing Data for Statistical Algorithm

Prior to carrying out our statistical algorithm, we split the data randomly into training and test data. 80% of the observations were used for the training dataset, while the remaining 20% were used for the test dataset.

```{r c7, include = F}
## Split the data into training and test set
set.seed(2345)
index <- sample(1:nrow(master2),round(0.80*nrow(master2)), replace=F) 
train.data <- as.data.frame(master2[index,])
test.data <- as.data.frame(master2[-index,])
names <- names(train.data)

# write formula using a function
f <- as.formula(paste("Target ~", paste(names[!names %in% "Target"], collapse = " + ")))
```

We then tested the correlation between pairs of the 60 predictor variables in our dataset in order to determine whether any of them were highly correlated. All variable pairs that had a correlation higher than 0.75 were dropped from the dataset. Afterwards, 49 variables remained in the dataset used for analysis. The variable names of the 49 indicators used in the analysis are shown below. 

```{r c8, include = F}
### test correlation to drop correlated variables
correlation <- train.data[,-2]
correlation[1:60] <- lapply(correlation[1:60], as.numeric)
cor.mat <- cor(correlation)
dropcor <- findCorrelation(cor.mat,cutoff = 0.75, verbose = FALSE, names = TRUE, exact = ncol(cor.mat) < 100)

#drop "floormosaic","rooms","avgeduadults","energyelec","totalmem","totperson","toiletsewer","waterindoor","elecpub","roofmetal"

train.data <- train.data %>%
  select(-c(all_of(dropcor)))
train.data <- train.data %>%
  select(-age) #drop age of head of household

#50 variables

# If you want to use later for smaller dataset - gives heatmap of correlation
#library(GGally)
#ggcorr(train.data)
```

```{r table1, echo=F}
varnames <- names(train.data[,-1])
varnames
```

# Regression Framework: Multinomial Logistic Regression 

Our response variable, `Target`, is a four-category variable that represents poverty levels. Given the composition of the response variable, we selected multinomial logistic regression for our statistical algorithm. Multinomial logistic regression does not assume that there is an order to the categories in the response variable. As a result, it is suitable for this study's response variable, whose categories do not have a specific order or rank. 

We first ran a multinomial logistic regression that regressed our response variable Target onto the 49 predictor variables in the training dataset. 

We then used stepwise regression (both forward and backward selection) on this model in order to select the predictor variables that were most relevant to our analysis. The best multinomial logistic regression using stepwise regression had an AIC of 4149.02. However, since this model includes 45 variables, we do not interpret this model given that there were still too many predictor variables. 

```{r, results='hide'}
#Multinomial Logistic Regression with all predictor variables 
set.seed(2345)
reg1 <- multinom(Target ~ ., data = train.data)

#Select Best Model using Stepwise Regression
reg2 <- stepAIC(reg1)

#Best multinomial logit regression using stepwise regression (AIC=4149.02)
reg3 <- multinom(Target ~ toilet + fridge + wallbrick + wallsocket + wallcement + 
    wallwaster + wallwood + wallzinc + wallnatural + wallother + 
    floorcement + floorother + floornatural + floorno + floorwood + 
    roofcement + roofnatural + roofother + ceiling + wateroutdoor + 
    waterno + elecpvt + elecno + eleccoop + toiletno + toiletseptic + 
    toilethole + toiletother + energyno + energygas + energywood + 
    children + adults + morethan65 + houseown + houseloan + houserented + 
    computer + qmobilephone + rcentral + rchorotega + rpaca + 
    rbrunca + rhetarat + eduhoh, data= train.data)
```

## Variable Selection with LASSO

We ran an elastic net regression with an alpha of 1, which is identical to running a LASSO regression. The LASSO method is preferred when a model has many independent variables, but few actually contribute to its performance.

We keep 8 variables, seen below, after running a LASSO regression. We found that wallbrick (if the predominant material on the outside wall of the home is block or brick), floorcement (if the predominant material on the floor of the home is cement), ceiling (if the home has a ceiling), energywood (if the main source of energy used for cooking was wood or charcoal), children (number of children ages 0 to 19 in the household), qmobilephone (number of mobile phones), rpaca (whether the home is located in the Pacific Central region of Costa Rica, the country's poorest region), and (eduhoh (education level for the head of household) were relevant variables to our analysis. We have also included a plot that shows the fit of the elastic net regression model. 

```{r, echo=F}
varnames8 <- c("wallbrick", "floorcement", "ceiling", "energywood", "children", "qmobilephone", "rpaca", "eduhoh")
kable(varnames8)
```

```{r, include=F}
#Setup for running Elastic Net Regression

#Predictor variables
x <- model.matrix(Target ~., train.data) [,-1]

#Outcome variable
#Make target an integer variable so that it can be used in the regressuib
Target <- as.integer(train.data$Target)
y <- Target

# Elastic Net Regression to find relevant coefficients for the model. We use an alpha of 1 so that the model is essentially a LASSO model. 
lasso <- cv.glmnet(x, y, alpha=1, nfolds=10) 

# Identify which coefficients are relevant
cs <- coefficients(lasso)
cs <- cs[which(cs!=0),]
rownames(as.matrix(cs))

#wallbrick, floorcement, ceiling, energywood, children, qmobilephone, rpaca,    eduhoh are relevant

# Plotting the fit of the model 
plot(lasso)
```

## Results of the Multinomial Logistic Regression

We run our final multinomial logistic regression on these 8 relevant variables. We report findings for statistically significant coefficients in each of the 4 categories in our response variable `Target`. Each category of the response variable was compared to the reference category, extreme poverty.

```{r, include = F}
#Multinomial Logistic Regression with relevant variables
reg4 <- multinom(Target ~ wallbrick + floorcement + ceiling + energywood + children + qmobilephone + rpaca + eduhoh, data = train.data)

output <- summary(reg4)
output

# Get p-values of coefficients
t.value <- output$coefficients/output$standard.errors
pval = round((pnorm(abs(t.value), lower.tail = FALSE) * 2),3)
pval
```

### Predicted Log Odds of Moderate versus Extreme Poverty

```{r, echo=F}
#table of coefficients and p-values, Moderate Poverty (Target=2)
reg4tab <- rbind(output$coefficients[1,],output$standard.errors[1,],t.value[1,],pval[1,])
rownames(reg4tab) <- c("Coefficient","Std. Errors","T Statistic","P-value")
kable(t(reg4tab),  caption = "Coefficients for Moderate vs. Extreme Poverty")%>%
kable_styling(latex_options = "hold_position")  
#Significant: children, qmobilephone
```

**Significant Predictors.**
The number of children ages 0 to 19 in the household and the number of mobile phones in the household were statistically significant predictors of the odds of being in moderate as opposed to extreme poverty. 

**Analysis.**
The log odds of being in moderate poverty versus extreme poverty will decrease by 0.125 as the number of children from ages 0 to 19 in the household increases. In other words, the more children are in the household, the lower the odds the household will be in moderate as opposed to extreme poverty. 

Further, the log odds of being in moderate poverty versus extreme poverty will increase by 0.238 as the number of moble phones in the household increases. The more mobile phones a household has, the higher the odds it will be in moderate as opposed to extreme poverty.

### Predicted Log Odds of Vulnerable Households versus Extreme Poverty

```{r, echo = F}
#table of coefficients and p-values, Vulnerable Households (Target = 3)
reg4tab3 <- rbind(output$coefficients[2,],output$standard.errors[2,],t.value[2,],pval[2,])
rownames(reg4tab3) <- c("Coefficient","Std. Errors","T Statistic","P-value")
kable(t(reg4tab3), caption= "Coefficients for Vulnerable HH vs. Extreme Poverty")%>%
kable_styling(latex_options = "hold_position")  
#Significant variables: children, qmobilephone
```

**Significant Predictors.**
The number of children ages 0 to 19 in the household and the number of mobile phones in the household were statistically significant predictors of the odds of being vulnerable to poverty as opposed to being in extreme poverty. 

**Analysis.**
The log odds of being a household vulnerable to poverty versus being in extreme poverty will decrease by 0.365 as the number of children from ages 0 to 19 in the household increases. In other words, the more children are in the household, the lower the odds the household will be vulnerable to poverty as opposed to being in extreme poverty. 

Finally, the log odds of being vulnerable to poverty versus being in extreme poverty will increase by 0.454 as the number of moble phones in the household increases. The more mobile phones a household has, the higher the odds it will be vulnerable to poverty as opposed to being in extreme poverty. 

### Predicted Log Odds of Non-Vulnerable Households versus Extreme Poverty*

```{r, echo = F}
#table of coefficients and p-values, Non-Vulnerable Households (Target = 4 )
reg4tab4 <- rbind(output$coefficients[3,],output$standard.errors[3,],t.value[3,],pval[3,])
rownames(reg4tab4) <- c("Coefficient","Std. Errors","T Statistic","P-value")
kable(t(reg4tab4), caption= "Coefficients for Non-Vulnerable HH vs. Extreme Poverty")%>%
kable_styling(latex_options = "hold_position")  
#Significant: wallbrick, ceiling, energywood, children, qmobilephone, rpaca, eduhoh 
```

**Significant Predictors.**
The number of children ages 0 to 19 in the household, the number of mobile phones in the household, whether the predominant material on the outside wall of the home was block or brick, whether there was a ceiling in the home, whether the main source of energy the household used for cooking was wood or charcoal, whether the home was in Costa Rica's Central Pacific region, and the head of household's years of education were statistically significant predictors of the odds of being a non-vulnerable household as opposed to being in extreme poverty. 

**Analysis.**
The log odds of not being vulnerable to poverty versus being in extreme poverty will decrease by 0.918 as the number of children from ages 0 to 19 in the household increases. The more children are in the household, the lower the odds the household will be non-vulnerable to poverty as opposed to being in extreme poverty. Families with fewer children will likely have more resources as opposed to those who have more children. Those with more children will have to spread their resources more thinly and may thus be more vulnerable to poverty as a result.

The log odds of not being vulnerable to poverty versus being in extreme poverty will increase by 0.721 as the number of moble phones in the household increases. The more mobile phones a household has, the higher the odds it will be non-vulnerable to poverty as opposed to being in extreme poverty. Families with more mobile phones may likely have more resources and as a result be less vulnerable to extreme poverty.

The log odds of not being vulnerable to poverty versus being in extreme poverty will increase by 0.528 if the predominant material on the outside wall of the home is block or brick. Brick and block are more expensive building materials than wood, for instance. Those who can afford homes made out of these materials are likely to have more money than those whose homes are not made out of these materials. In turn, they would likley be less vulnerable to extreme poverty than those who have fewer resources.  

The log odds of not being vulnerable to poverty versus being in extreme poverty will increase by 0.524 if there is a ceiling in the home. Those who have a ceiling are more likely to have the resources that would protect them from falling into extreme poverty.

The log odds of not being vulnerable to poverty versus being in extreme poverty will decrease by 0.750 if the main source of energy the household uses for cooking is wood or charcoal. Households that use less expensive sources of energy to cook such as wood or charcoal would have fewer resources than a household that uses gas or electricity for cooking. Thus, they would have lower odds of not being vulnerable to poverty as opposed to being in extreme poverty.

The log odds of not being vulnerable to poverty versus being in extreme poverty will decrease by 0.736 if the home is in the Central Pacific region, which is the nation's poorest area. Homes in this area will in turn have lower odds of not being vulnerable to poverty as opposed to being in extreme poverty.

The log odds of not being vulnerable to poverty versus being in extreme poverty will increase by 0.161 for each one-year increase in the head of the household's education. The more educated the head of the household is, the higher the odds the household will not be vulnerable to poverty as opposed to being in extreme poverty.

## Multinomial Logistic Regression Model Prediction Accuracy 

We found that the multinomial logistic model correctly predicted 67.68% of the observations in the test dataset. However, as we can see from the confusion matrix, most of the correctly categorized households were those that were non-vulnerable. Within the other three categories, the majority of households were actually incorrectly misclassified as non-vulnerable as well. We will attempt to remedy this through machine learning methods.

```{r, include = F}
#Predictions for the test data
predicted_poverty <- predict(reg4, test.data) 

#Prediction accuracy - confusion matrix
testtab <- table(predicted_poverty, test.data$Target) 
testtab

#Prediction Accuracy Calculation
0+24+0+378
402/594 #67.68%
```

```{r, echo=F}
#Print Confusion Matrix Table
rownames(testtab) <- c("Extreme Pov.","Moderate Pov.","Vulnerable HH","Non-Vulnerable HH")
colnames(testtab) <- c("Extreme Pov.","Moderate Pov.","Vulnerable HH","Non-Vulnerable HH")
kable(testtab, caption = "Confusion Matrix for MLR: Predicted Poverty Level")%>%
kable_styling(latex_options = "hold_position")  
```

# Machine Learning: Random Forest

For the machine learning portion of the project, we decide to perform a random forest classification algorithm. Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction than just a single decision tree.

## Standard Random Forest

We first tune the mtry parameter which is the number of variables randomly sampled as candidates at each split. As seen in the plot below, using a 10-fold cross validation and growing 500 trees in our random forest, we find that a mtry of 7 gives us the least Out of Bag (OOB) error. 

```{r random forest, echo = F, message=FALSE}
## tuning mtry
tuneRF(train.data[,-1], train.data[,1], ntreeTry = 500)
```

Using the parameters `mtry = 7` and `ntree = 500`, I fit a full random tree classification model on 50 variables. While the overall OOB error is 0.33, the error for 3 of the 4 classes (extreme poverty, moderate poverty,and vulnerable households) is actually quite large. The error for extreme poverty households is 0.93, the error for moderate poverty is 0.77, and the error for vulnerable households is 0.97. The lower overall OOB error is mainly due to the low OOB error of 0.05 for non-vulnerable households. These errors are plotted below and further confirmed in the confusion matrix of the predicted and the actual classifiers in our training data. 

We find that most households were classified as non-vulnerable. For extreme poverty households, 66.1% were misclassifed as non-vulnerable.For moderate poverty, 72.7% were misclassified as non-vulnerable. And for vulnerable households, 81.7% were misclassified as non-vulnerable. Thus, the majority of households who should have qualified for social programs were classified as not qualifying for these programs.

Since our goal is to help the IADB predict which households qualify for social programs, a model that classifies non-vulnerable households correctly, but misclassifies all vulnerable and poor households is not useful. To adjust for this, we choose to create a model using the balanced random forest methodology proposed by Chen, Liaw, and Brieman (2004).

```{r, include = F}
fit.rf <- randomForest(Target~., train.data, ntree=500, proximity=T, importance=T, mtry=7)
```

```{r, include = F}
#Predict Output for train
predict.train <- predict(fit.rf, type="response")
mean(train.data$Target != predict.train) #training error

summary(gg_error(fit.rf)) #error rates for each classification
```

```{r, echo = F}
cm <- confusionMatrix(predict.train, train.data$Target)
# extract the confusion matrix values as data.frame
cm_d <- as.data.frame(cm$table)
# here we also have the rounded percentage values
cm_p <- as.data.frame(prop.table(cm$table,2))
cm_d$Perc <- round(cm_p$Freq*100,2)

# plotting the matrix
cm_d_p <-  ggplot(data = cm_d, aes(x = Prediction , y =  Reference, fill = Perc))+
  geom_tile() +
  ggtitle("Standard Random Forest: Predicted vs. Actual Classifiers") +
  geom_text(aes(label = paste("",Freq,",",Perc,"%")), color = 'red', size = 3.5) +
  theme_light() +
  guides(fill=FALSE) 

plot(gg_error(fit.rf), main="Standard Random Forest: Plot of OOB Error Rate")
```

```{r,echo=F}
cm_d_p
```

## Balanced Random Forest

In learning extremely imbalanced data, there is a significant probability that a bootstrap sample contains few or even none of the minority classes, resulting in a tree with poor performance for predicting them. A way to address this is to down-sample which involves sampling the majority class at frequencies closer to the rarest class. While a down-side to down-sampling is that information in the majority class is being thrown away, random forest models allow us to use down-sampling without data loss since we are bootstrapping many samples and aggregating the results. 

We follow Chen, Liaw, and 'Brieman's (2004) Balanced Random Forest (BRF) algorithm. Instead of bootstrapping our samples from the entire dataset, as done in the classical random forest method, we sample by strata (each classification) and restrict their sample size to our smallest class which is households in extreme poverty.


```{r, include = F}
nmin <- sum(train.data$Target == "1")
rf.bal <- randomForest(Target~., train.data,
                       ntree=500, 
                       proximity=T,
                       importance=T, 
                       mtry=7,
                       strata = train.data$Target,
                       sampsize = rep(nmin,4))
```

```{r}
predict.train.bal <- predict(rf.bal, type="response")
mean(train.data$Target != predict.train.bal) #training error
```

Using BRF gives us a training error of 0.47. This overall OOB error rate is higher than the 0.33 error rate in the previous model. However, the OOB Error plot shows that the errors for the vulnerable household categories are lower than before. The error for extreme poverty is 0.74, for moderate poverty is 0.60, and for vulnerable households is 0.73. The error for non-vulnerable households increased to 0.35. While the overall OOB error is higher than the standard random forest, it is much lower for the households of interest - extremely poor, moderately poor, and vulnerable. 

```{r, include = F}
summary(gg_error(rf.bal))
```


```{r, echo = F}
cm2 <- confusionMatrix(predict.train.bal, train.data$Target)
# extract the confusion matrix values as data.frame
cm_d2 <- as.data.frame(cm2$table)
# here we also have the rounded percentage values
cm_p2 <- as.data.frame(prop.table(cm2$table,2))
cm_d2$Perc <- round(cm_p2$Freq*100,2)

# plotting the matrix
cm_d_p2 <-  ggplot(data = cm_d2, aes(x = Prediction , y =  Reference, fill = Perc))+
  geom_tile() +
  ggtitle("Balanced Random Forest: Predicted vs. Actual Classifiers") +
  geom_text(aes(label = paste("",Freq,",",Perc,"%")), color = 'red', size = 3.5) +
  theme_light() +
  guides(fill=FALSE) 
plot(gg_error(rf.bal), main="Standard Random Forest: Plot of OOB Error Rate")
```

```{r,echo=F}
cm_d_p2
```


### BRF Final Model

To avoid overfitting our model, we decided to simplify the balanced random forest model by choosing variables with the most predictive power. 

We perform a 10 fold crossvalidation of our balanced random forest model using the `rcfv` function. In this process, random forest models with sequentially reduced number of predictors are generated. The predictors are ordered by importance, and an error rate is estimated for each model.  As seen in the plot below, the "best model" balancing parsimony and the accuracy rate is a model with 7 variables. 

According to the mean accuracy rate, the 7 variables with most prediction power (highest importance) are the years of education of the head of household (`eduhoh`), number of children in the household (`children`), if the house has a ceiling (`ceiling`), number of adults in the household (`adults`), if the predominant material on the floor is cement (`floorcement`), number of mobile phones in the household (`qmobilephone`), and if the predominant material on the outside wall is block or brick (`wallbrick`).

We see overlapping important variables with our logistic regression - `wall brick`, `floorcement`, `eduhoh`, `children`, `ceiling`, and `qmobilephone` - indicating that across models these variables are important for prediction of the four categories.

```{r, echo=F}
fo <- rfcv(train.data[,-1],train.data[,1], cv.fold=10, scale="log", step=0.75)

best <- which.max(fo$error.cv)

plot( fo$n.var,fo$error.cv, type = "h", main = "importance", 
      xlab= "number of indicators", ylab = "classifier error rate")
axis(1, best, paste("best", best, sep="\n"), col = "red", col.axis = "red")
```

```{r, include = F}
#looking at variable importance
round(importance(rf.bal), 2)
```

```{r, echo = F}
varImpPlot(rf.bal,  
           sort = T,
           n.var=7,
           main="Top 7 - Variable Importance")

```

```{r, include = F}
nmin <- sum(train.data$Target == "1")
rf.bal.7 <- randomForest(Target~eduhoh+children+ceiling+adults+floorcement+qmobilephone+wallbrick, train.data,
                       ntree=500, 
                       proximity=T,
                       importance=T, 
                       mtry=7,
                       strata = train.data$Target,
                       sampsize = rep(nmin,4))
```

We fit a model to the 7 most "important" predictors and test the model on our test data. Using this model, we have an overall testing error of 47% which gives us a prediction accuracy rate of 53%.  While this error rate is very large, this model accurately predicts extreme poverty households at 33.3%, moderate poverty at 35.4% and vulnerable households at 22.1%. From the confusion matrix shows us that of all our models, this model predicts vulnerable, moderate, and extreme poverty households the most accurately.

```{r}
#predict output for test
predict.test <- predict(rf.bal.7, newdata = test.data)
mean(test.data$Target != predict.test) #testing error
```

```{r, echo=F}
cm2 <- confusionMatrix(predict.test, test.data$Target)
# extract the confusion matrix values as data.frame
cm_d2 <- as.data.frame(cm2$table)
# here we also have the rounded percentage values
cm_p2 <- as.data.frame(prop.table(cm2$table,2))
cm_d2$Perc <- round(cm_p2$Freq*100,2)

# plotting the matrix
cm_d_p2 <-  ggplot(data = cm_d2, aes(x = Prediction , y =  Reference, fill = Perc))+
  geom_tile() +
  ggtitle("Final Model on Test Data: Predicted vs. Actual Classifiers") +
  geom_text(aes(label = paste("",Freq,",",Perc,"%")), color = 'red', size = 3.5) +
theme_light() +
  guides(fill=FALSE) 
cm_d_p2
```

# Conclusion

```{r, echo = F}
options(digits=3)
MLR <- c(67.6, 0, 30.4, 0, 96.2)
SRF <- c(67.7, 7.3, 23.4, 3.2, 95.9)
BRF <- c(52.5, 33.3, 35.4, 22.1, 64.1)

concl <- rbind(MLR, SRF, BRF)
colnames(concl) <- c("Total", "Extreme Pov", "Moderate Pov", "Vulnerable HH", "Non-Vulnerable HH")

kable(concl) %>%
  kable_styling(latex_options = "hold_position")
```

From these models, we see that both the mulinomial logistc regression and standard random forest have similar prediction accuracy overall. However, when we look at the prediction accuracy within each category, we see that this high accuracy is maily due to the accuracy of classification for non-vulnerable households only. The largest share of the households in our dataset are non-vulnerable households; thus, skewing our models in favor of classifying these households accurately. However, even when we compare the logistic regression to the standard random forest, we see that the random forest is better able to classify some households in the categories of extreme poverty and vulnerable.

Since the objective of is to help the IADB predict which households qualify for social programs, a model that misclassifies all vulnerable and poor households is not useful. To adjust for the disproportionate number of non-vulnerable households in our dataset, we implemented a balanced random forest which samples by each category and restricts the sample size of each to the smallest category, which in our case was households in extreme poverty.

Using the balanced random forest, we had an lower overall prediction accuracy rate at 52.5%. However, within each category of interest, the prediction rate is much higher. About a third of households in extreme poverty or moderate poverty were accurately classified and more than 20% of vulnerable households were classified correctly.

In addition, with our objective in mind, the misclassification of households into the non-vulnerable category is especially harmful. For the standard random forest, more 70% of extremely poor, moderately poor, and vulnerable households were misclassifed as non-vulnerable. This is especially undesirable in our model since this misclassification error means that 70% of the households in our data that need aid from social programs will not receive any at all since they are seen as "non-vulnerable". 

While the balanced random forest model is still only predicting accurately at a rate around 30% for extremely poor, moderately poor, and vulnerable households, their misclassification as a non-vulnerable household is much lower. Only 15% of extremely poor households are misclassified as non-vulnerable, only 20% for moderately poor, and 27% for vulnerable. Thus, we determine that the balanced random forest model is the better model for identifying which households qualify for social programs if our goal is to provide families with aid they need. Even if extremely poor households are misclassified, they are more often misclassified as moderately poor. Even if moderately poor households are misclassified, they have a higher probability of being categorized as extremely poor or vulnerable rather than as a non-vulnerable household. While the overall prediction accuracy is lower in a balanced random forest model, we determine that that it is the best model for the objective of this project.

